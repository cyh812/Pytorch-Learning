{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensor**是pytorch中的重要变量，包含具体的数值和梯度两部分，当loss执行backward时梯度部分会计算loss对于w权重的偏导\n",
    "**LongTensor**常常用于离散的数据和标签；部分时候输入要求必须是**LongTensor**，如果不对会报错，改正即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.Tensor([1.0])\n",
    "w.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失**loss**执行反向传播通过backward函数实现，注意该函数在没有添加智能补全插件的其前提下不会出现补全提示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取**Tensor**中的数值，可以直接访问（得到tensor类型数据），或者加上**item()**转化为int或float类型的数值，当前**grad**需要执行反向传播才会有数据否则报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor data: tensor([[-0.8346, -0.1115,  0.0908],\n",
      "        [-0.3981,  1.4364, -0.9879],\n",
      "        [ 2.0842, -0.6768,  1.3137]], requires_grad=True)\n",
      "tensor([[-0.8346, -0.1115,  0.0908],\n",
      "        [-0.3981,  1.4364, -0.9879],\n",
      "        [ 2.0842, -0.6768,  1.3137]], requires_grad=True)\n",
      "tensor([[-0.8346, -0.1115,  0.0908],\n",
      "        [-0.3981,  1.4364, -0.9879],\n",
      "        [ 2.0842, -0.6768,  1.3137]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(3, 3, requires_grad=True)\n",
    "print(\"Tensor data:\", x)\n",
    "\n",
    "# 使用 x 直接访问数据\n",
    "print(x)\n",
    "\n",
    "# 或者通过 .data 访问\n",
    "print(x.data)\n",
    "\n",
    "print(x.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度**grad**在每次执行完权重更新后需要归零，否则pytorch默认的计算方式是梯度累加的（当然部分模型结构确实是用累加梯度的）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.grad.data.zero_() #我自定义一个tensor作为权重w（可视为一个极其简单的无偏置网络层）并将梯度归零\n",
    "optimizer.zero_grad() #实际使用过程中会定义优化器optimizer及其用zero_grad()方法归零权重"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.nn.Linear(m, n)**包含权重和偏置，权重为[n,m]的矩阵，偏置为一维的[n,1]/[n,]\n",
    "\n",
    "**forward**是魔法函数，简单来说，后期如果需要调用实例化的模型进行前馈计算，不需要**model.forward(x)**这种形式而是直接**model(x)**即可\n",
    "\n",
    "**sigmod/ReLU**这类激活函数，可以来自于**torch.nn.functional**也可以直接**torch.nn.Sigmoid()**调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearModel, self).__init__() #just do it\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "model = LinearModel()\n",
    "\n",
    "# 查看具体参数\n",
    "print('w = ', model.linear.weight.item())\n",
    "print('b = ', model.linear.bias.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义损失函数与优化器，替代了我们自行定义设计的损失函数和梯度下降函数（一般为最小二乘法那个的导数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(reduction='sum')  # 或 'mean'，视需要而定\n",
    "criterion = torch.nn.BCELoss(reduction='sum') # 交叉熵损失\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练基本流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "\n",
    "    y_pred = model(x_data) #准备输入数据和标签\n",
    "\n",
    "    loss = criterion(y_pred, y_data) #使用criterion计算损失\n",
    "\n",
    "    optimizer.zero_grad() #权重梯度归零；部分情况可能不需要\n",
    "\n",
    "    loss.backward() #计算损失，本质上损失的数值不重要，重要的是通过这一步完成计算图的反向传播构建，并赋予optimizer新的权重梯度\n",
    "\n",
    "    optimizer.step() #这一步是完成权重更新的计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取/下载数据\n",
    "1.pytorch中自带的数据集下载\n",
    "**transform= transforms.ToTensor()**用于将数据转为**Tensor**\n",
    "2.自己的数据导入\n",
    "要想将自定义的 Dataset 作为 DataLoader 的输入参数，必须确保它实现了以下两个方法：\n",
    "__getitem__(self, index)：这个方法应该返回数据集的每个样本。返回值通常是一个元组 (x, y)，其中 x 是输入数据，y 是标签或目标数据。通过这个方法，DataLoader 可以按需获取每一批次的数据。\n",
    "__len__(self)：返回数据集的总大小。DataLoader 需要知道数据集的大小来进行分批次加载。\n",
    "只要这两个方法符合要求，Dataset 就可以作为 DataLoader 的输入。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(root='./dataset/mnist', train=True, download=True)\n",
    "test_set = torchvision.datasets.MNIST(root='./dataset/mnist', train=False, download=True)\n",
    "# train_set = torchvision.datasets.CIFAR10(…)\n",
    "# test_set = torchvision.datasets.CIFAR10(…)\n",
    "test_set = torchvision.datasets.MNIST(root='./dataset/mnist', train=False,transform= transforms.ToTensor(), download=True)\n",
    "\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __getitem__(self, index):\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        pass\n",
    "dataset = DiabetesDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                            batch_size=32,\n",
    "                            shuffle=True,\n",
    "                            num_workers=2) #启用多线程读取，该维度一般建议删除会报错\n",
    "\n",
    "# 往往配合如下\n",
    "for epoch in range(100):\n",
    "    for i, data in enumerate(train_loader, 0):  #如果默认从第一个batch读取，0可以删除\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "断点测试模型各层维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(batch_size,\n",
    "in_channels,\n",
    "width, \n",
    "height)\n",
    "# 生成指定维度的输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于**Softmax**的逻辑补充，如下图所示\n",
    "\n",
    "![image.png](Softmax.png)\n",
    "\n",
    "分类计算的结果经过转换后与目标标签的one-hot编码进行交叉熵计算，本质上只有one-hot值为1的维度及其对应的概率值进行了有效计算，也就是损失就是只有如下一个数值\n",
    "$$ -\\log(\\hat{Y}) $$\n",
    "\n",
    "仔细比较下方手撸的交叉熵和官方提供的交叉熵会发现，实际计算过程中目标标签只需要给出一个值而不是一个one-hot的数组即可。因为交叉熵函数会根据给出的值（就是具体代表的类别）作为下标去取预测的数组中对应维度的那个值，然后就足以计算损失了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = np.array([1, 0, 0])\n",
    "z = np.array([0.2, 0.1, -0.1])\n",
    "y_pred = np.exp(z) / np.exp(z).sum()\n",
    "loss = (- y * np.log(y_pred)).sum()\n",
    "print(loss)\n",
    "\n",
    "y = torch.LongTensor([0])\n",
    "z = torch.Tensor([[0.2, 0.1, -0.1]])\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "loss = criterion(z, y)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练前对数据归一化的好处\n",
    "在深度学习中，**正态分布**或接近 **标准正态分布**（均值为 0，标准差为 1）的数据对加速训练过程非常有利。这与模型的优化、梯度下降算法的表现以及神经网络的训练过程密切相关。以下是一些关键原因，解释为什么数据符合正态分布能够加速训练：\n",
    "\n",
    "### 1. **避免梯度爆炸或梯度消失**\n",
    "神经网络在训练过程中依赖反向传播算法计算梯度，而梯度的大小直接影响权重更新的步长。\n",
    "\n",
    "- **梯度爆炸**：如果输入数据的范围太大，网络中的梯度在反向传播过程中可能会变得非常大，导致权重更新过大，进而导致数值不稳定，甚至无法收敛。\n",
    "- **梯度消失**：如果输入数据的范围过小，梯度可能会变得非常小，导致权重更新非常缓慢，甚至没有有效的更新。\n",
    "\n",
    "通过将数据转换为零均值、单位方差的分布（即标准正态分布），可以有效地避免这两种问题。标准化后的数据避免了输入数据的极端值，从而减少了梯度爆炸或梯度消失的风险，使得训练过程更加稳定。\n",
    "\n",
    "### 2. **加快收敛速度**\n",
    "当输入数据符合正态分布时，神经网络模型的训练过程往往能更快地收敛。这是因为：\n",
    "\n",
    "- **标准化后的数据具有统一的尺度**：标准化后，所有输入特征的范围变得一致（均值为 0，标准差为 1）。这意味着在梯度下降过程中，每个特征对梯度的贡献相对均衡，避免了某些特征对模型训练产生过大的影响。\n",
    "- **梯度下降更加高效**：统一的尺度让优化器（如SGD、Adam等）能够更均匀地更新网络权重，从而更高效地找到最优解。否则，如果特征尺度不一致，优化器可能会沿某些维度“过冲”或“滞后”，导致收敛速度减慢。\n",
    "\n",
    "### 3. **提高模型的鲁棒性**\n",
    "神经网络模型的鲁棒性是指其在面对不同训练数据或输入时的表现稳定性。标准化后的数据：\n",
    "- **避免模型对某些特征过度敏感**：如果某些特征的数值范围过大，网络可能会对这些特征过度敏感，而忽视其他重要特征。通过标准化，所有特征的影响力被平衡，使模型能够全面地学习每个特征。\n",
    "- **提高泛化能力**：数据的标准化有助于减少过拟合，并使模型更容易在不同数据分布下表现得更好。标准化后的数据通常使模型在新数据上的表现更加稳定，具有更好的泛化能力。\n",
    "\n",
    "### 4. **平衡权重更新**\n",
    "神经网络中的每一层通常由许多神经元组成，每个神经元的输出通常是前一层输出的加权和（加上偏置）。如果输入数据的分布不均匀，某些神经元的加权和可能会非常大或非常小，从而导致训练时权重更新的不平衡。标准化数据确保所有神经元在训练开始时得到一个均衡的起点，避免了部分神经元在训练过程中过度主导模型更新。\n",
    "\n",
    "### 5. **对优化算法的适应性**\n",
    "许多优化算法（如 Adam, RMSprop, etc.）在数据标准化时表现得更好。这些优化器通常会根据输入数据的特性（如均值、方差）调整学习率和梯度的更新步伐。通过对数据进行标准化处理，优化算法能够更高效地适应不同的任务和数据特性，从而提高训练速度。\n",
    "\n",
    "### 6. **减少学习率调节的需求**\n",
    "未标准化的数据可能需要更复杂的学习率调节机制，以便避免训练过程中出现过大的梯度更新。标准化后的数据通常允许你使用一个固定的学习率，而不需要频繁调整，从而简化了超参数的调节过程。\n",
    "\n",
    "---\n",
    "\n",
    "### 总结\n",
    "符合正态分布的数据对于加速训练非常有利，因为它能：\n",
    "- **避免梯度爆炸和梯度消失**，确保梯度计算更加稳定；\n",
    "- **加速收敛**，通过标准化后的数据，优化器能够更高效地进行权重更新；\n",
    "- **提高模型的鲁棒性**，减少对单个特征的过度依赖，提升模型在不同数据集上的泛化能力；\n",
    "- **平衡权重更新**，避免某些神经元过度主导训练过程；\n",
    "- **简化优化算法的调节**，减少学习率调节的复杂性。\n",
    "\n",
    "因此，标准化处理（尤其是标准正态分布的处理）是深度学习中数据预处理的一个重要步骤，能够显著提升模型训练的效率和效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m transform \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      2\u001b[0m transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m      3\u001b[0m transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.1307\u001b[39m, ), (\u001b[38;5;241m0.3081\u001b[39m, ))\n\u001b[0;32m      4\u001b[0m ])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize((0.1307, ), (0.3081, ))\n",
    "])\n",
    "\n",
    "# 以MNIST为例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在测试阶段使用**with torch.no_grad():**避免梯度计算减少不必要的开销"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于CNN的一些说明\n",
    "- 如果一个input图像的通道数为n，那么其用于卷积运算的卷积核的通道数也是n，这两个数值的一样的。因为每个通道所进行的卷积的参数是不一样的要一一对应。\n",
    "- 通道为n的input与通道为n的卷积核进行计算后，得到的output通道数为1，因为每个通道卷积运算后还需要相加最后合并成一个通道\n",
    "- 因此，常见的input通过卷积后反而更厚了（通道数更多了），是因为参与计算的卷积核有好几个，卷积核的数量m才是output的通道数\n",
    "\n",
    "如果想要输入输出的图像尺寸（长和宽）不变，可以\n",
    "- 卷积层添加padding：**conv_layer = torch.nn.Conv2d(1, 1, kernel_size=3, padding=1, bias=False)**\n",
    "- 使用1*1卷积，不改变尺寸但是可以改变通道数，卷积层为（输入通道数，输出通道数，1，1）\n",
    "\n",
    "卷积运算做到最后还是一个batch\\*channel\\*weight\\*height的“块”，为了得到输出还得展平成二维（[batch,...]）的矩阵接一个线性层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = torch.nn.Conv2d(1, 1, kernel_size=3, stride=2, bias=False)\n",
    "# 关键参数 输入通道，输出通道，卷积核尺寸；这里的通道不知道是不是可以理解为卷积里的特征（好像又不太合适）\n",
    "# padding和stride，bias可选\n",
    "maxpooling_layer = torch.nn.MaxPool2d(kernel_size=2)\n",
    "# 池化层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用GPU，注意除了把模型送到GPU，训练/测试过程中的数据也要送到GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net() #实例化一个模型\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 简单一点 device = torch.device(\"cuda:0\")\n",
    "model.to(device)\n",
    "\n",
    "# 把数据也送到GPU\n",
    "inputs, target = inputs.to(device), target.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GoogleNet架构，注意其通过1*1卷积和padding的控制使得输入输出的通道数变化（24，16，24，24），但是尺寸大小（w\\*h）是不变的，最后再进行拼接（\n",
    "**outputs = [branch1x1, branch5x5, branch3x3,branch_pool] return torch.cat(outputs, dim=1)**），拼接的时候**dim=1**指围绕channel拼接，因为第一维是batch\n",
    "\n",
    "![image.png](GoogleNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于RNN的一些说明\n",
    "- 在RNN里面，首先有一个输入维度seq\\*batchsize\\*inputsize，这里一般seq放在最前面需要想清楚，因为输入的数据有好几批，但是RNN计算还有一个序列顺序，也就是说从seq中取batch批次的每份数据中时间t序列的值，这样就好理解了\n",
    "- 还有一个隐藏维度hiddensize，在RNN中其维度为batchsize\\*hiddensize，这里很不一样的点在于这个batchsize在之前的网络中虽然也会涉及，但是不会作为一个要**数值对齐的维度**，但是RNN中要\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNNCell和RNN的区别\n",
    "# 1.RNNCell只有一层，所以少一个layer参数，而且输入不要求seq这个维度，导致需要我们在外面自己包一层for循环来完成整体的计算，此外它只有一个输出即hidden，代表特定时刻下对应输入的输出\n",
    "# 2.RNNCell内部可以扩充层数；输入多要求了seq维度，所以内部封装了循环，不需要我们自己在外面额外写了；因为内部有了循环，所以输出有两个，一个是最终输出的hidden，和RNNCell一样（多了layer层数这一维度），另外一个是每个时刻的hidden的汇总（因为内部围绕seq作了循环，所以拼接一下就有了）\n",
    "\n",
    "# 如果希望batchsize放前面，可以加一个参数 batch_first=True\n",
    "\n",
    "cell = torch.nn.RNNCell(input_size=input_size, hidden_size=hidden_size)\n",
    "# input of shape 𝑏𝑎𝑡𝑐ℎ, 𝑖𝑛𝑝𝑢𝑡_𝑠𝑖𝑧𝑒\n",
    "# hidden of shape 𝑏𝑎𝑡𝑐ℎ, ℎ𝑖𝑑𝑑𝑒𝑛_𝑠𝑖𝑧e\n",
    "\n",
    "# hidden of shape 𝑏𝑎𝑡𝑐ℎ, ℎ𝑖𝑑𝑑𝑒𝑛_𝑠𝑖𝑧e\n",
    "\n",
    "cell = torch.nn.RNN(input_size=input_size, hidden_size=hidden_size,\n",
    "num_layers=num_layers)\n",
    "# input of shape 𝑠𝑒𝑞𝑆𝑖𝑧𝑒, 𝑏𝑎𝑡𝑐ℎ, 𝑖𝑛𝑝𝑢𝑡_𝑠𝑖𝑧𝑒\n",
    "# hidden of shape 𝑛𝑢𝑚𝐿𝑎𝑦𝑒𝑟𝑠, 𝑏𝑎𝑡𝑐ℎ, ℎ𝑖𝑑𝑑𝑒𝑛_𝑠𝑖𝑧e\n",
    "\n",
    "# output of shape 𝑠𝑒𝑞𝑆𝑖𝑧𝑒, 𝑏𝑎𝑡𝑐ℎ, 𝑖𝑛𝑝𝑢𝑡_𝑠𝑖𝑧𝑒\n",
    "# hidden of shape 𝑛𝑢𝑚𝐿𝑎𝑦𝑒𝑟𝑠, 𝑏𝑎𝑡𝑐ℎ, ℎ𝑖𝑑𝑑𝑒𝑛_𝑠𝑖𝑧e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于**torch.nn.Embedding(input_size, embedding_size)**的理解。inputsize代表词汇量的总数（比如我准备的数据集一共有1w个单词那就是10000），embeddingsize代码用几维的向量表示这1w个词汇（不再是one-hot那种，每个维度可以是任意值0，0.1，0.001等等所以绝对是可以不重复表示完全的）\n",
    "在使用的时候，我有形如（batch，seq）的输入，在x = self.emb(x)后的输出为（batch，seq，embedding_size）即根据每个batch中每个seq（索引）取出对应数据的embedding向量（大小为embedding_size）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LiMu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
