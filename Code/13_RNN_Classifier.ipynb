{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Parameters\n",
    "HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 256\n",
    "N_LAYER = 2\n",
    "N_EPOCHS = 100\n",
    "N_CHARS = 128\n",
    "USE_GPU = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameDataset(Dataset):\n",
    "    def __init__(self, is_train_set=True):\n",
    "        filename = 'data/names_train.csv.gz' if is_train_set else 'data/names_test.csv.gz'\n",
    "        with gzip.open(filename, 'rt') as f:\n",
    "            reader = csv.reader(f)\n",
    "            rows = list(reader)  # 数据结构为 [name,country]\n",
    "        self.names = [row[0] for row in rows] # 得到姓名列表[name1,name2,name3...]\n",
    "        self.len = len(self.names) # 姓名/数据集总数\n",
    "        self.countries = [row[1] for row in rows] # 得到国家列表\n",
    "        self.country_list = list(sorted(set(self.countries))) # 国家列表去重再降序排序\n",
    "        self.country_dict = self.getCountryDict() # 转化为字典 country:id\n",
    "        self.country_num = len(self.country_list) # 国家总数\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.names[index], self.country_dict[self.countries[index]]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def getCountryDict(self):\n",
    "        country_dict = dict()\n",
    "        for idx, country_name in enumerate(self.country_list, 0):\n",
    "            country_dict[country_name] = idx\n",
    "        return country_dict\n",
    "    def idx2country(self, index): # 基于索引返回国家\n",
    "        return self.country_list[index]\n",
    "    def getCountriesNum(self):\n",
    "        return self.country_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = NameDataset(is_train_set=True)\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testset = NameDataset(is_train_set=False)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "N_COUNTRY = trainset.getCountriesNum() #获得国家数，也就是类别数，也是最后模型输出的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size #这里即输出维度\n",
    "        self.n_layers = n_layers # 网络层数\n",
    "        self.n_directions = 2 if bidirectional else 1 # 是否双向\n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size) # inputsize为，hiddensize同上，为输出的维度，这里是把希望的嵌入向量的维度（即输入维度）和输出维度设置为一样了\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                bidirectional=bidirectional) # 因为嵌入向量的维度（即输入维度）和输出维度设置为一样，所以两个都用hiddensize了\n",
    "        self.fc = torch.nn.Linear(hidden_size * self.n_directions, output_size) # outputsize应该和N_COUNTRY是一致的\n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers * self.n_directions,\n",
    "                batch_size, self.hidden_size)\n",
    "        return create_tensor(hidden)\n",
    "\n",
    "    def forward(self, input, seq_lengths):\n",
    "        # input shape : B x S -> S x B\n",
    "        input = input.t() # 输入前自己有数是B x S还是 S x B吧\n",
    "        batch_size = input.size(1) # 取batchsize，仅仅用于初始化隐藏层\n",
    "        \n",
    "        hidden = self._init_hidden(batch_size)\n",
    "        embedding = self.embedding(input) #得到初步输入 seq * batchsize * hiddensize\n",
    "       \n",
    "        # pack them up\n",
    "        gru_input = pack_padded_sequence(embedding, seq_lengths) #一个特殊的打包处理，反正是有利于数据计算加速的\n",
    "        # 这里的embedding来自于input的转化，而input是在该类之外进行处理的（完成了0填充补全）\n",
    "        # seq_lengths同样也是在外面处理好再输入的，以张量的形式按顺序存储input的每个batch的长度\n",
    "        \n",
    "        output, hidden = self.gru(gru_input, hidden) # 这里的gru_input不用管太多，pack_padded_sequence会自动处理成RNN类能计算的格式\n",
    "        if self.n_directions == 2:\n",
    "            hidden_cat = torch.cat([hidden[-1], hidden[-2]], dim=1)\n",
    "        else:\n",
    "            hidden_cat = hidden[-1]\n",
    "        fc_output = self.fc(hidden_cat)\n",
    "        return fc_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 制作输入数据，这个才是最麻烦的\n",
    "def make_tensors(names, countries): #names和countries为list形式的数据和标签索引\n",
    "    sequences_and_lengths = [name2list(name) for name in names]\n",
    "    # name2list 首先将每个名字里的一个个字母提取出来变为字母列表，并且还返回列表长度，整体数据为\n",
    "    # [(['a','b',....],3),([],len),([],len)...] 大概长这样\n",
    "\n",
    "    name_sequences = [sl[0] for sl in sequences_and_lengths] #sl为([],len)\n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequences_and_lengths])\n",
    "    countries = countries.long() #转变为longtensor\n",
    "    # name_sequences——[['a','b','c'],[],[]...] 大概长这样\n",
    "    # seq_lengths——[1,2,3,4,5...]\n",
    "\n",
    "\n",
    "    # make tensor of name, BatchSize x SeqLen\n",
    "    seq_tensor = torch.zeros(len(name_sequences), seq_lengths.max()).long() # 得到全为0的batch*seq的longtensor，注意这里的seq是最长的那个名字，完成了用0补全\n",
    "    for idx, (seq, seq_len) in enumerate(zip(name_sequences, seq_lengths), 0):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq) # 用真实数据一个个覆盖全0的数据\n",
    "    # sort by length to use pack_padded_sequence\n",
    "    seq_lengths, perm_idx = seq_lengths.sort(dim=0, descending=True) #训练数据长度重排\n",
    "    seq_tensor = seq_tensor[perm_idx] #根据重排后的索引顺序调整训练数据顺序\n",
    "    countries = countries[perm_idx] #根据重排后的索引顺序调整标签顺序\n",
    "    return create_tensor(seq_tensor), \\\n",
    "        create_tensor(seq_lengths),\\\n",
    "        create_tensor(countries)\n",
    "\n",
    "    \"\"\"\n",
    "    def create_tensor(tensor):\n",
    "    if USE_GPU:\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    tensor = tensor.to(device)\n",
    "    return tensor\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel():\n",
    "    total_loss = 0\n",
    "    for i, (names, countries) in enumerate(trainloader, 1):\n",
    "        inputs, seq_lengths, target = make_tensors(names, countries)\n",
    "        output = classifier(inputs, seq_lengths)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print(f'[{time_since(start)}] Epoch {epoch} ', end='')\n",
    "            print(f'[{i * len(inputs)}/{len(trainset)}] ', end='')\n",
    "            print(f'loss={total_loss / (i * len(inputs))}')\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel():\n",
    "    correct = 0\n",
    "    total = len(testset)\n",
    "    print(\"evaluating trained model ...\")\n",
    "    with torch.no_grad():\n",
    "        for i, (names, countries) in enumerate(testloader, 1):\n",
    "            inputs, seq_lengths, target = make_tensors(names, countries)\n",
    "            output = classifier(inputs, seq_lengths)\n",
    "            pred = output.max(dim=1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        percent = '%.2f' % (100 * correct / total)\n",
    "        print(f'Test set: Accuracy {correct}/{total} {percent}%')\n",
    "    return correct / total\n",
    "\n",
    "\"\"\"\n",
    "output.max(dim=1, keepdim=True)：\n",
    "\n",
    "dim=1：表示沿着 列（类别维度）进行排序，即对于每一行（每个样本），找到最大的预测值。\n",
    "keepdim=True：表示保持原有的维度。默认情况下，max 操作会将某个维度压缩掉（即减少一个维度）。如果 keepdim=True，则输出的维度保持一致，只不过会将该维度的大小变为 1。这样能保留输出形状，避免后续的形状错误。\n",
    "结果会返回一个包含两个元素的元组：\n",
    "第一个元素是 每行的最大值（即最大得分），形状为 (batch_size, 1)。\n",
    "第二个元素是 最大值的索引，即每个样本的预测类别索引，形状为 (batch_size, 1)。\n",
    "[1]：我们需要的是 最大值的索引，而不是最大得分，所以通过 [1] 来提取第二个元素，即 预测的类别索引。所以 pred 的形状为 (batch_size, 1)，表示每个样本的预测类别。\n",
    "\n",
    "pred.eq(target.view_as(pred))：\n",
    "pred 是模型的预测类别索引，形状为 (batch_size, 1)。\n",
    "target 是实际的标签，通常是一个张量，表示每个样本的真实类别。其形状也是 (batch_size, 1)。\n",
    "target.view_as(pred)：将 target 的形状调整为与 pred 相同的形状。这里 view_as(pred) 只是确保 target 和 pred 的形状一致，避免维度不匹配。\n",
    "pred.eq(target.view_as(pred))：这是一个元素级的比较操作，会返回一个布尔张量，其中每个元素是 True 或 False，表示每个样本的预测是否与真实标签匹配。结果的形状为 (batch_size, 1)。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRY, N_LAYER)\n",
    "    if USE_GPU:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        classifier.to(device)\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "    \n",
    "    # start = time.time() \n",
    "    print(\"Training for %d epochs...\" % N_EPOCHS)\n",
    "    acc_list = []\n",
    "    for epoch in range(1, N_EPOCHS + 1):\n",
    "        # Train cycle\n",
    "        trainModel()\n",
    "        acc = testModel()\n",
    "        # acc_list.append(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LiMu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
